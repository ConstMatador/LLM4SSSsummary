{
    "model_selected": "TimeLLM",

    "conf_path": "./conf/GPT4SSS.json",
    "data_path": "../Data/SEAnet/data_250M_seed1184_len256_znorm.bin",
    "llm_path": "../Model/",
    "train_path": "./example/GPT4SSS/sample/train.bin",
    "val_path": "./example/GPT4SSS/sample/val.bin",
    "test_path": "./example/GPT4SSS/sample/test.bin",
    "train_indices_path": "./example/GPT4SSS/sample/train_indices.bin",
    "val_indices_path": "./example/GPT4SSS/sample/val_indices.bin",
    "test_indices_path": "./example/GPT4SSS/sample/test_indices.bin",
    "log_path": "./example/GPT4SSS/log/example.log",
    "model_path": "./example/GPT4SSS/model/",

    "device": "cuda:3",
    "GPUs": [3, 4],
    
    "llm_type": "gpt2",
    "llm_layers": 6,

    "epoch_max": 100,
    "len_series": 256,
    "len_reduce": 16,
    "batch_size": 128,
    
    "data_size": 1000000,
    "train_size":200000,
    "val_size": 10000,
    "test_size": 10000,

    "patch_len": 16,
    "stride": 8,

    "head_num": 8,
    "dim_model": 128,
    "dim_llm": 768,
    "dim_ff": 128,

    "reduced_vocab_size": 1000,

    "seq_overview": "These sequences come from the salt dataset, with a sequence length of 256. We need to compress them into sequences of length 16 and try to keep the original sequence features.",

    "prompt_len": 256
}
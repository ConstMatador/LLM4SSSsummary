{
    "model_selected": "TimeLLM",
    "dataset_selected": "deep1B",

    "conf_path": "./conf/TimeLLM.json",
    "data_path": "../Data/LLM4SSS/",
    "llm_path": "../Model/",
    "train_path": "./example/TimeLLM/sample/train.bin",
    "val_path": "./example/TimeLLM/sample/val.bin",
    "test_path": "./example/TimeLLM/sample/test.bin",
    "train_indices_path": "./example/TimeLLM/sample/train_indices.bin",
    "val_indices_path": "./example/TimeLLM/sample/val_indices.bin",
    "test_indices_path": "./example/TimeLLM/sample/test_indices.bin",
    "log_path": "./example/TimeLLM/log/example.log",
    "model_path": "./example/TimeLLM/model/",

    "device": "cuda:6",
    "GPUs": [6, 7],
    
    "llm_type": "gpt2",
    "llm_layers": 6,

    "epoch_max": 100,
    "len_series": 96,
    "len_reduce": 16,
    "batch_size": 512,
    
    "data_size": 1000000,
    "train_size":200000,
    "val_size": 10000,
    "test_size": 10000,

    "patch_len": 16,
    "stride": 8,

    "head_num": 8,
    "dim_model": 128,
    "dim_llm": 768,
    "dim_ff": 128,

    "reduced_vocab_size": 1000,

    "seq_overview": "These sequences have a length of 96. We need to compress them into sequences of length 16 and try to keep the original sequence features.",

    "prompt_len": 256
}